{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script does the following:\n",
    "\n",
    "Converts each PDF page to an image.\n",
    "Sends each image to the Azure Document Intelligence API for text extraction.\n",
    "Collects and saves the extracted text to a .txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "401 Client Error: Access Denied for url: https://cursive-handwritings.cognitiveservices.azure.com/vision/v3.2/read/analyze",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 48\u001b[0m\n\u001b[0;32m     45\u001b[0m image_bytes \u001b[38;5;241m=\u001b[39m page\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Analyze the handwriting in the image\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_handwriting\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Extract text from the JSON response\u001b[39;00m\n\u001b[0;32m     51\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[8], line 19\u001b[0m, in \u001b[0;36manalyze_handwriting\u001b[1;34m(image_bytes)\u001b[0m\n\u001b[0;32m     17\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mendpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/vision/v3.2/read/analyze\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Endpoint for the 'read' API\u001b[39;00m\n\u001b[0;32m     18\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(url, headers\u001b[38;5;241m=\u001b[39mheaders, data\u001b[38;5;241m=\u001b[39mimage_bytes)\n\u001b[1;32m---> 19\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Retrieve the operation location for polling\u001b[39;00m\n\u001b[0;32m     22\u001b[0m operation_location \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mheaders[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOperation-Location\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\herpa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1019\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1020\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1021\u001b[0m     )\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 401 Client Error: Access Denied for url: https://cursive-handwritings.cognitiveservices.azure.com/vision/v3.2/read/analyze"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pdf2image import convert_from_path\n",
    "import time\n",
    "\n",
    "# Azure Document Intelligence endpoint and API key\n",
    "endpoint = \"https://cursive-handwritings.cognitiveservices.azure.com\"  # Replace with your endpoint\n",
    "api_key = \"\"  # Replace with your API key\n",
    "\n",
    "# Headers for the API request\n",
    "headers = {\n",
    "    \"Ocp-Apim-Subscription-Key\": api_key,\n",
    "    \"Content-Type\": \"application/octet-stream\"\n",
    "}\n",
    "\n",
    "# Function to analyze handwritten text in an image\n",
    "def analyze_handwriting(image_bytes):\n",
    "    url = f\"{endpoint}/vision/v3.2/read/analyze\"  # Endpoint for the 'read' API\n",
    "    response = requests.post(url, headers=headers, data=image_bytes)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    # Retrieve the operation location for polling\n",
    "    operation_location = response.headers[\"Operation-Location\"]\n",
    "    \n",
    "    # Poll the operation location to check if processing is complete\n",
    "    while True:\n",
    "        result_response = requests.get(operation_location, headers={\"Ocp-Apim-Subscription-Key\": api_key})\n",
    "        result = result_response.json()\n",
    "        \n",
    "        if result.get(\"status\") == \"succeeded\":\n",
    "            return result\n",
    "        elif result.get(\"status\") == \"failed\":\n",
    "            raise Exception(\"Handwriting analysis failed.\")\n",
    "        \n",
    "        time.sleep(1)  # Wait a second before polling again\n",
    "\n",
    "# Convert PDF to images and perform OCR on each page\n",
    "pdf_path = \"Example 1.pdf\"  # Replace with your PDF file path\n",
    "output_file = \"handwritten_text_results.txt\"\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "    pages = convert_from_path(pdf_path, dpi=300)\n",
    "    \n",
    "    for page_num, page in enumerate(pages, start=1):\n",
    "        # Convert page image to bytes\n",
    "        image_bytes = page.tobytes()\n",
    "        \n",
    "        # Analyze the handwriting in the image\n",
    "        result = analyze_handwriting(image_bytes)\n",
    "        \n",
    "        # Extract text from the JSON response\n",
    "        text = \"\"\n",
    "        for read_result in result[\"analyzeResult\"][\"readResults\"]:\n",
    "            for line in read_result[\"lines\"]:\n",
    "                text += line[\"text\"] + \"\\n\"\n",
    "        \n",
    "        # Write the text to the output file\n",
    "        file.write(f\"Text from page {page_num}:\\n{text}\\n{'-'*80}\\n\")\n",
    "        \n",
    "        print(f\"Text from page {page_num} saved.\")\n",
    "\n",
    "print(f\"OCR results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code sample shows Prebuilt Read operations with the Azure Form Recognizer client library. \n",
    "The async versions of the samples require Python 3.6 or later.\n",
    "\n",
    "To learn more, please visit the documentation - Quickstart: Document Intelligence (formerly Form Recognizer) SDKs\n",
    "https://learn.microsoft.com/azure/ai-services/document-intelligence/quickstarts/get-started-sdks-rest-api?pivots=programming-language-python\n",
    "\"\"\"\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "\n",
    "\"\"\"\n",
    "Remember to remove the key from your code when you're done, and never post it publicly. For production, use\n",
    "secure methods to store and access your credentials. For more information, see \n",
    "https://docs.microsoft.com/en-us/azure/cognitive-services/cognitive-services-security?tabs=command-line%2Ccsharp#environment-variables-and-application-configuration\n",
    "\"\"\"\n",
    "endpoint = \"YOUR_FORM_RECOGNIZER_ENDPOINT\"\n",
    "key = \"YOUR_FORM_RECOGNIZER_KEY\"\n",
    "\n",
    "def format_bounding_box(bounding_box):\n",
    "    if not bounding_box:\n",
    "        return \"N/A\"\n",
    "    return \", \".join([\"[{}, {}]\".format(p.x, p.y) for p in bounding_box])\n",
    "\n",
    "def analyze_read():\n",
    "    # sample document\n",
    "    formUrl = \"https://raw.githubusercontent.com/Azure-Samples/cognitive-services-REST-api-samples/master/curl/form-recognizer/sample-layout.pdf\"\n",
    "\n",
    "    document_analysis_client = DocumentAnalysisClient(\n",
    "        endpoint=endpoint, credential=AzureKeyCredential(key)\n",
    "    )\n",
    "    \n",
    "    poller = document_analysis_client.begin_analyze_document_from_url(\n",
    "            \"prebuilt-read\", formUrl)\n",
    "    result = poller.result()\n",
    "\n",
    "    print (\"Document contains content: \", result.content)\n",
    "    \n",
    "    for idx, style in enumerate(result.styles):\n",
    "        print(\n",
    "            \"Document contains {} content\".format(\n",
    "                \"handwritten\" if style.is_handwritten else \"no handwritten\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    for page in result.pages:\n",
    "        print(\"----Analyzing Read from page #{}----\".format(page.page_number))\n",
    "        print(\n",
    "            \"Page has width: {} and height: {}, measured with unit: {}\".format(\n",
    "                page.width, page.height, page.unit\n",
    "            )\n",
    "        )\n",
    "\n",
    "        for line_idx, line in enumerate(page.lines):\n",
    "            print(\n",
    "                \"...Line # {} has text content '{}' within bounding box '{}'\".format(\n",
    "                    line_idx,\n",
    "                    line.content,\n",
    "                    format_bounding_box(line.polygon),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        for word in page.words:\n",
    "            print(\n",
    "                \"...Word '{}' has a confidence of {}\".format(\n",
    "                    word.content, word.confidence\n",
    "                )\n",
    "            )\n",
    "\n",
    "    print(\"----------------------------------------\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph 1:\n",
      "  Found '/s:' (similarity: 90)\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 2:\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 3:\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 4:\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 5:\n",
      "  Found 'a' (similarity: 90)\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 6:\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 7:\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 8:\n",
      "  Found 'A' (similarity: 90)\n",
      "  Found 'a' (similarity: 90)\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 9:\n",
      "  Found 'A' (similarity: 90)\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 10:\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 11:\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 12:\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 13:\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 14:\n",
      "  Found 'a.' (similarity: 90)\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 15:\n",
      "  Found 'a' (similarity: 90)\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 16:\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 17:\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 18:\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 19:\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 20:\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 21:\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 22:\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 23:\n",
      "  Found 'A' (similarity: 90)\n",
      "  Found 'a' (similarity: 90)\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 24:\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 25:\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 26:\n",
      "  Found 'a' (similarity: 90)\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 27:\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 28:\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 29:\n",
      "  Found 'a' (similarity: 90)\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 30:\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 31:\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 32:\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 33:\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 34:\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 35:\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 36:\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 37:\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 38:\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 39:\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 40:\n",
      "  Found 'a' (similarity: 90)\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 41:\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 42:\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 43:\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 44:\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 45:\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 46:\n",
      "  Found 'ad' (similarity: 90)\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 47:\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 48:\n",
      "  Found 'a' (similarity: 90)\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 49:\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 50:\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 51:\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 52:\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 53:\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 54:\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 55:\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 56:\n",
      "  Found 'a' (similarity: 90)\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 57:\n",
      "  Found 'A' (similarity: 90)\n",
      "  Found 'a' (similarity: 90)\n",
      "  Found 'ad' (similarity: 90)\n",
      "  Found 's' (similarity: 90)\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 58:\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 59:\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 60:\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 61:\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 62:\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 63:\n",
      "  Found 'a' (similarity: 90)\n",
      "  Found ''s' (similarity: 90)\n",
      "  Found 'dres' (similarity: 90)\n",
      "  Found 'a' (similarity: 90)\n",
      "  Found 'A' (similarity: 90)\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 64:\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 65:\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 66:\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Paragraph 67:\n",
      "\n",
      "------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz, process\n",
    "\n",
    "# Sample dictionary of words you're interested in\n",
    "# target_words = [\"Hampshire\", \"Sheriff\", \"Alexander\", \"Court\", \"Goods\", \"Common Pleas\", \"address\"]\n",
    "\n",
    "target_words = [\"address\"]\n",
    "\n",
    "# Function to read OCR text from a file\n",
    "def load_text_from_file(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return file.read()\n",
    "\n",
    "# Function to search for target words with strict fuzzy matching\n",
    "def search_words_in_text(ocr_text, target_words, threshold):\n",
    "    found_words = {}\n",
    "    \n",
    "    # Split the OCR text into paragraphs or sections based on your separator\n",
    "    paragraphs = ocr_text.split(\"==========\")\n",
    "    \n",
    "    # Process each paragraph\n",
    "    for i, paragraph in enumerate(paragraphs, start=1):\n",
    "        paragraph = paragraph.strip()\n",
    "        found_words[i] = []\n",
    "        \n",
    "        # Check each word in the dictionary\n",
    "        for word in target_words:\n",
    "            # Find close matches in the paragraph with a strict threshold\n",
    "            matches = process.extractBests(word, paragraph.split(), score_cutoff=threshold, limit=5)\n",
    "            \n",
    "            # Save matches that meet the strict threshold\n",
    "            for match in matches:\n",
    "                found_words[i].append((match[0], match[1]))\n",
    "    \n",
    "    return found_words\n",
    "\n",
    "# Load the OCR text from a .txt file\n",
    "ocr_text = load_text_from_file(\"document_intelligence_example1.txt\")  # Replace with your actual file path\n",
    "\n",
    "# Perform the search with a stricter threshold\n",
    "results = search_words_in_text(ocr_text, target_words, threshold=90)\n",
    "\n",
    "# Display results\n",
    "for paragraph_num, matches in results.items():\n",
    "    print(f\"Paragraph {paragraph_num}:\")\n",
    "    for match in matches:\n",
    "        print(f\"  Found '{match[0]}' (similarity: {match[1]})\")\n",
    "    print(\"\\n\" + \"-\"*30 + \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
